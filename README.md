# LLM finetuning
# Language Model Finetuning with Unsloth

This notebook demonstrates finetuning two different language models, Qwen 2 7B and CodeGemma 7B, using the Unsloth library for faster and more memory-efficient finetuning.

## Unsloth Library

[Unsloth](https://github.com/unslothai/unsloth) is a library that provides optimizations for finetuning large language models, resulting in faster training and reduced memory usage. It achieves this through various techniques like kernel optimizations and efficient memory management. This notebook leverages Unsloth to finetune both Qwen 2 and CodeGemma models effectively on limited resources.

## Setup

The notebook starts by installing the necessary libraries, including `unsloth`, `bitsandbytes`, `accelerate`, `peft`, `trl`, and `datasets`. This is handled in the initial setup cells.



### Model Loading and PEFT Setup

The Qwen 2 7B model is loaded using `FastLanguageModel.from_pretrained` with 4-bit quantization, which is optimized by Unsloth for faster loading and lower memory footprint. PEFT adapters are then applied using `FastLanguageModel.get_peft_model`, leveraging Unsloth's optimizations for PEFT training.

## Setup

The notebook starts by installing the necessary libraries for finetuning large language models, including `unsloth`, `bitsandbytes`, `accelerate`, `peft`, `trl`, and `datasets`. This is handled in the initial setup cells of the notebook.

The `%%capture` magic command is used in the installation cell to suppress the extensive output generated by the installation process, keeping the notebook clean.

The following code snippet shows the installation process, including a conditional check for Google Colab environments to ensure the correct dependencies are installed:

```python
%%capture
import os
if "COLAB_" not in "".join(os.environ.keys()):
    !pip install unsloth
else:
    # Do this only in Colab notebooks! Otherwise use pip install unsloth
    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo
    !pip install sentencepiece protobuf "datasets>=3.4.1" huggingface_hub hf_transfer
    !pip install --no-deps unsloth
```
### Data Preparation

The dataset for finetuning is loaded using the `load_dataset` function from the `datasets` library. In this case, the `yahma/alpaca-cleaned` dataset is used.

A formatting function, `formatting_prompts_func`, is defined to structure the instruction, input, and output from the dataset into a prompt format suitable for training. The `EOS_TOKEN` is added to the end of each formatted text to indicate the end of a sequence. The dataset is then mapped using this function.
python
alpaca_prompt = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{}

### Input:
{}

### Response:
{}"""

EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN
def formatting_prompts_func(examples):
    instructions = examples["instruction"]
    inputs       = examples["input"]
    outputs      = examples["output"]
    texts = []
    for instruction, input, output in zip(instructions, inputs, outputs):
        # Must add EOS_TOKEN, otherwise your generation will go on forever!
        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN
        texts.append(text)
    return { "text" : texts, }
pass

from datasets import load_dataset
dataset = load_dataset("yahma/alpaca-cleaned", split = "train")
dataset = dataset.map(formatting_prompts_func, batched = True,)

### Training

The finetuning process is carried out using the `SFTTrainer` from the `trl` library. The trainer is configured with various parameters specified in the `TrainingArguments`. Key parameters include `per_device_train_batch_size`, `gradient_accumulation_steps`, `warmup_steps`, `max_steps`, `learning_rate`, and the optimizer (`adamw_8bit`). The trainer is then started using `trainer.train()`.
python
from trl import SFTTrainer
from transformers import TrainingArguments
from unsloth import is_bfloat16_supported

trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset,
    dataset_text_field = "text",
    max_seq_length = max_seq_length,
    dataset_num_proc = 2,
    args = TrainingArguments(
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 4,

        # Use num_train_epochs = 1, warmup_ratio for full training runs!
        warmup_steps = 5,
        max_steps = 60,

        learning_rate = 2e-4,
        fp16 = not is_bfloat16_supported(),
        bf16 = is_bfloat16_supported(),
        logging_steps = 1,
        optim = "adamw_8bit",
        weight_decay = 0.01,
        lr_scheduler_type = "linear",
        seed = 3407,
        output_dir = "outputs",
        report_to = "none", # Use this for WandB etc
    ),
)

trainer_stats = trainer.train()

### Inference

After finetuning, the model can be used for inference. `FastLanguageModel.for_inference(model)` is called to enable Unsloth's optimized inference. The input prompt is formatted using the `alpaca_prompt` and the tokenizer. The `model.generate` function is used to generate text based on the input. The `TextStreamer` can be used to stream the generated output.
python
# alpaca_prompt = Copied from above
FastLanguageModel.for_inference(model) # Unsloth has 2x faster inference!
inputs = tokenizer(
[
    alpaca_prompt.format(
        "Continue the fibonnaci sequence.", # instruction
        "1, 1, 2, 3, 5, 8", # input
        "", # output - leave this blank for generation!
    )
], return_tensors = "pt").to("cuda")

outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)
tokenizer.batch_decode(outputs)
python
# alpaca_prompt = Copied from above
FastLanguageModel.for_inference(model) # Unsloth has 2x faster inference!
inputs = tokenizer(
[
    alpaca_prompt.format(
        "Continue the fibonnaci sequence.", # instruction
        "1, 1, 2, 3, 5, 8", # input
        "", # output - leave this blank for generation!
    )
], return_tensors = "pt").to("cuda")

from transformers import TextStreamer
text_streamer = TextStreamer(tokenizer)
_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)

### Saving the Model

The finetuned LoRA adapters can be saved locally using `model.save_pretrained("lora_model")` and the tokenizer using `tokenizer.save_pretrained("lora_model")`.
python
model.save_pretrained("lora_model")  # Local saving
tokenizer.save_pretrained("lora_model")

### WandB Logging

Weights & Biases (WandB) is used to log the training process and save the finetuned model as an artifact. After logging in to WandB using `wandb.login()`, a run is initialized with `wandb.init()`. An artifact is created to store the finetuned model directory, and the artifact is logged to the WandB run.
python
import wandb
wandb.login()

run = wandb.init(project="qwen2-finetuning", job_type="upload-model")

# Create a W&B artifact for the model
artifact = wandb.Artifact(
    name="qwen2-lora-model",
    type="model",
    description="LoRA adapters for Qwen2-7B finetuned on Alpaca dataset",
)

# Add the model directory to the artifact
artifact.add_dir("lora_model")

# Log the artifact
run.log_artifact(artifact)

# Finish the WandB run
run.finish()

## CodeGemma 7B Finetuning

### Model Loading and PEFT Setup

The CodeGemma 7B model is loaded using `FastLanguageModel.from_pretrained` with 4-bit quantization for efficiency. PEFT adapters are then applied using `FastLanguageModel.get_peft_model`, utilizing Unsloth's optimizations for PEFT training.
python
from unsloth import FastLanguageModel
import torch

max_seq_length = 4096  # Gemma sadly only supports max 8192 for now
dtype = (
    None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+
)
load_in_4bit = True  # Use 4bit quantization to reduce memory usage. Can be False.

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/codegemma-7b-bnb-4bit",  # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
    # token = "hf_...", # use one if using gated models like meta-llama/Llama-2-7b-hf
)

model = FastLanguageModel.get_peft_model(
    model,
    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 16,
    lora_dropout = 0, # Supports any, but = 0 is optimized
    bias = "none",    # Supports any, but = "none" is optimized
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
    random_state = 3407,
    use_rslora = False,  # We support rank stabilized LoRA
    loftq_config = None, # And LoftQ
)

### Data Preparation

The `philschmid/guanaco-sharegpt-style` dataset is loaded and prepared for training. A chat template, specifically the `chatml` template in this case, is applied to format the conversations into a suitable text format for the model. A formatting function is used to apply this template to batches of data.
python
from unsloth.chat_templates import get_chat_template
from datasets import load_dataset

tokenizer = get_chat_template(
    tokenizer,
    chat_template = "chatml", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth
    mapping = {"role" : "from", "content" : "value", "user" : "human", "assistant" : "gpt"}, # ShareGPT style
    map_eos_token = True, # Maps <|im_end|> to </s> instead
)

def formatting_prompts_func(examples):
    convos = examples["conversations"]
    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]
    return { "text" : texts, }
pass

dataset = load_dataset("philschmid/guanaco-sharegpt-style", split = "train")
dataset = dataset.map(formatting_prompts_func, batched = True,)

### Training

The `SFTTrainer` from the `trl` library is used for finetuning the CodeGemma model. The training is configured using `SFTConfig`, specifying parameters like batch size, gradient accumulation steps, warmup steps, maximum steps, learning rate, and the optimizer.
python
from trl import SFTTrainer, SFTConfig

trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset,
    packing = False,  # Can make training 5x faster for short sequences.
    args = SFTConfig(
        per_device_train_batch_size = 1,
        gradient_accumulation_steps = 4,
        warmup_steps = 5,
        max_steps = 20,
        learning_rate = 2e-4,
        optim = "adamw_8bit",
        weight_decay = 0.01,
        lr_scheduler_type = "linear",
        seed = 3407,
        dataset_text_field = "text",
        report_to = "none",  # Use this for WandB etc
        max_grad_norm = 0.3,
    ),
)

trainer_stats = trainer.train()

### Inference

After finetuning, inference can be performed using the model. `FastLanguageModel.for_inference(model)` enables Unsloth's optimized inference. The input messages are formatted using `tokenizer.apply_chat_template`, and `model.generate` is used to produce the output. The `TextStreamer` can be used for streaming the output.
python
from unsloth.chat_templates import get_chat_template
from transformers import TextStreamer

tokenizer = get_chat_template(
    tokenizer,
    chat_template = "chatml", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth
    mapping = {"role" : "from", "content" : "value", "user" : "human", "assistant" : "gpt"}, # ShareGPT style
    map_eos_token = True, # Maps <|im_end|> to </s> instead
)

FastLanguageModel.for_inference(model) # Enable native 2x faster inference

messages = [
    {"from": "human", "value": "Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,"},
]
inputs = tokenizer.apply_chat_template(
    messages,
    tokenize = True,
    add_generation_prompt = True, # Must add for generation
    return_tensors = "pt",
).to("cuda")

outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True)
tokenizer.batch_decode(outputs)

FastLanguageModel.for_inference(model) # Enable native 2x faster inference

messages = [
    {"from": "human", "value": "Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,"},
]
inputs = tokenizer.apply_chat_template(
    messages,
    tokenize = True,
    add_generation_prompt = True, # Must add for generation
    return_tensors = "pt",
).to("cuda")

text_streamer = TextStreamer(tokenizer)
_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128, use_cache = True)

### Saving the Model

The finetuned LoRA adapters for the CodeGemma model can be saved locally using `model.save_pretrained("lora_model")`.
python
model.save_pretrained("lora_model")  # Local saving
# Merging and Quantization

After finetuning the LoRA adapters, you can merge them with the base model or quantize the model for more efficient deployment. Unsloth provides convenient methods for these operations.

### Merging LoRA Adapters

You can merge the finetuned LoRA adapters with the base model into different formats using `model.save_pretrained_merged` or push them to the Hugging Face Hub using `model.push_to_hub_merged`.

To merge to a 16-bit format:python
# Merge to 16bit
if False: model.save_pretrained_merged("model", tokenizer, save_method = "merged_16bit",)
if False: model.push_to_hub_merged("hf/model", tokenizer, save_method = "merged_16bit", token = "")

To merge to a 4-bit format:python
# Merge to 4bit
if False: model.save_pretrained_merged("model", tokenizer, save_method = "merged_4bit",)
if False: model.push_to_hub_merged("hf/model", tokenizer, save_method = "merged_4bit", token = "")

You can also save or push just the LoRA adapters without merging:python
# Just LoRA adapters
if False:
    model.save_pretrained("model")
    tokenizer.save_pretrained("model")
if False:
    model.push_to_hub("hf/model", token = "")
    tokenizer.push_to_hub("hf/model", token = "")

### Quantization

Unsloth supports various quantization methods for the merged model using `model.save_pretrained_gguf` or `model.push_to_hub_gguf`.

To save to 8-bit Q8_0 GGUF format:python
# Save to 8bit Q8_0
if False: model.save_pretrained_gguf("model", tokenizer,)
if False: model.push_to_hub_gguf("hf/model", tokenizer, token = "")

To save to 16-bit GGUF format:python
# Save to 16bit GGUF
if False: model.save_pretrained_gguf("model", tokenizer, quantization_method = "f16")
if False: model.push_to_hub_gguf("hf/model", tokenizer, quantization_method = "f16", token = "")

To save to q4_k_m GGUF format:python
# Save to q4_k_m GGUF
if False: model.save_pretrained_gguf("model", tokenizer, quantization_method = "q4_k_m")
if False: model.push_to_hub_gguf("hf/model", tokenizer, quantization_method = "q4_k_m", token = "")

Colab Notebook :
https://colab.research.google.com/drive/1XTETt9ZmcZc_Kuo-logdgK8G3ivO2Fm1
